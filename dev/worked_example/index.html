<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>A worked example ¬∑ DynamicHMC.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DynamicHMC.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li class="current"><a class="toctext" href>A worked example</a><ul class="internal"></ul></li><li><a class="toctext" href="../interface/">Documentation</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>A worked example</a></li></ul><a class="edit-page" href="https://github.com/tpapp/DynamicHMC.jl/blob/master/docs/src/worked_example.md"><span class="fa">ÔÇõ</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>A worked example</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="A-worked-example-1" href="#A-worked-example-1">A worked example</a></h1><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>An extended version of this example can be found <a href="https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_independent_bernoulli.jl">in the DynamicHMCExamples.jl package</a>.</p></div></div><p>Consider estimating estimating the parameter <span>$0 \le \alpha \le 1$</span> from <span>$n$</span> IID observations</p><div>\[y_i \sim \mathrm{Bernoulli}(\alpha)\]</div><p>We will code this with the help of TransformVariables.jl, and obtain the gradient with ForwardDiff.jl (in practice, for nontrivial models, at the moment I would recommend <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>).<a href="#footnote-4">[4]</a></p><div class="footnote" id="footnote-4"><a href="#footnote-4"><strong>[4]</strong></a><p>Note that NUTS is not especially suitable for low-dimensional parameter spaces, but this example works fine.</p></div><p>First, we load the packages we use.</p><pre><code class="language-julia">using TransformVariables, LogDensityProblems, DynamicHMC,
    DynamicHMC.Diagnostics, Parameters, Statistics, Random</code></pre><p>Generally, I would recommend defining defining an immutable composite type (ie <code>struct</code>) to hold the data and all parameters relevant for the log density (eg the prior). This allows you to test your code in a modular way before sampling. For this model, the number of draws equal to <code>1</code> is a sufficient statistic.</p><pre><code class="language-julia">struct BernoulliProblem
    n::Int # Total number of draws in the data
    s::Int # Number of draws `==1` in the data
end</code></pre><p>Then we make this problem <em>callable</em> with the parameters. Here, we have a single parameter <code>Œ±</code>, but pass this in a <code>NamedTuple</code> to demonstrate a generally useful pattern. Then, we define an instance of this problem with the data, called <code>p</code>.<a href="#footnote-5">[5]</a></p><div class="footnote" id="footnote-5"><a href="#footnote-5"><strong>[5]</strong></a><p>Note that here we used a <em>flat prior</em>. This is generally not a good idea for variables with non-finite support: one would usually make priors parameters of the <code>struct</code> above, and add the log prior to the log likelihood above.</p></div><pre><code class="language-julia">function (problem::BernoulliProblem)(Œ∏)
    @unpack Œ± = Œ∏               # extract the parameters
    @unpack n, s = problem       # extract the data
    # log likelihood, with constant terms dropped
    s * log(Œ±) + (n-s) * log(1-Œ±)
end</code></pre><p>It is generally a good idea to test that your code works by calling it with the parameters; it should return a likelihood. For more complex models, you should benchmark and <a href="https://docs.julialang.org/en/v1/manual/performance-tips/">optimize</a> this callable directly.</p><pre><code class="language-julia">p = BernoulliProblem(20, 10)
p((Œ± = 0.5, )) # make sure that it works</code></pre><pre><code class="language-none">-13.862943611198906</code></pre><p>With TransformVariables.jl, we set up a <em>transformation</em> <span>$\mathbb{R} \to [0,1]$</span> for <span>$\alpha$</span>, and use the convenience function <code>TransformedLogDensity</code> to obtain a log density in <span>$\mathbb{R}^1$</span>. Finally, we obtain a log density that supports gradients using automatic differentiation.</p><pre><code class="language-julia">trans = as((Œ± = asùïÄ,))
P = TransformedLogDensity(trans, p)
‚àáP = ADgradient(:ForwardDiff, P)</code></pre><pre><code class="language-none">ForwardDiff AD wrapper for TransformedLogDensity of dimension 1, w/ chunk size 1</code></pre><p>Finally, we run MCMC with warmup. Note that you have to specify the <em>random number generator</em> explicitly ‚Äî this is good practice for parallel code. The last parameter is the number of samples.</p><pre><code class="language-julia">results = mcmc_with_warmup(Random.GLOBAL_RNG, ‚àáP, 1000; reporter = NoProgressReport())</code></pre><p>The returned value is a <code>NamedTuple</code>. Most importantly, it contains the field <code>chain</code>, which is a vector of vectors. You should use the transformation you defined above to retrieve the parameters (here, only <code>Œ±</code>). We display the mean here to check that it was recovered correctly.</p><pre><code class="language-">posterior = transform.(trans, results.chain)
posterior_Œ± = first.(posterior)
mean(posterior_Œ±)</code></pre><p>Using the <a href="../interface/#Diagnostics-1"><code>DynamicHMC.Diagnostics</code></a> submodule, you can obtain various useful diagnostics. The <em>tree statistics</em> in particular contain a lot of useful information about turning, divergence, acceptance rates, and tree depths for each step of the chain. Here we just obtain a summary.</p><pre><code class="language-julia">summarize_tree_statistics(results.tree_statistics)</code></pre><pre><code class="language-none">Hamiltonian Monte Carlo sample of length 1000
  acceptance rate mean: 0.93, 5/25/50/75/95%: 0.73 0.9 0.98 1.0 1.0
  termination: divergence =&gt; 0%, max_depth =&gt; 0%, turning =&gt; 100%
  depth: 0 =&gt; 0%, 1 =&gt; 61%, 2 =&gt; 39%</code></pre><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>Usually one would run parallel chains and check convergence and mixing using generic MCMC diagnostics not specific to NUTS. See <a href="https://github.com/tpapp/MCMCDiagnostics.jl">MCMCDiagnostics.jl</a> for an implementation of <span>$\hat{R}$</span> and effective sample size calculations.</p></div></div><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Introduction</span></a><a class="next" href="../interface/"><span class="direction">Next</span><span class="title">Documentation</span></a></footer></article></body></html>
